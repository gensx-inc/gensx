---
title: Tools
description: Using tools with GenSX
---

# Using tools with GenSX

Workflows often require LLMs to interact with external systems or perform specific actions and using tools with LLMs is a powerful way to accomplish that.

This guide will show examples of using tools with both [@gensx/openai](../component-reference/openai.mdx) and [@gensx/vercel-ai](../component-reference/vercel-ai.mdx). You can also find similar examples in [OpenAI examples](https://github.com/gensx-inc/gensx/tree/main/examples/openai-examples) and [Vercel AI examples](https://github.com/gensx-inc/gensx/tree/main/examples/vercel-ai) in the GitHub repo.

## Tools with the Vercel AI SDK

The [`@gensx/vercel-ai`](../component-reference/vercel-ai.mdx) package provides a simple way to define and use tools with LLMs.

### Defining a tool

Start by defining your tool using the `tool` helper from the Vercel AI SDK:

```ts
import { tool } from "ai";
import { z } from "zod";

const weatherTool = tool({
  description: "Get the weather in a location",
  parameters: z.object({
    location: z.string().describe("The location to get the weather for"),
  }),
  execute: async ({ location }: { location: string }) => {
    console.log("Executing weather tool with location:", location);
    await new Promise((resolve) => setTimeout(resolve, 100));
    return {
      location,
      temperature: 72 + Math.floor(Math.random() * 21) - 10,
    };
  },
});
```

### Using tools with `generateText`

You can use tools with the `generateText` function by passing them in the `tools` prop:

```ts
const WeatherAssistant = gensx.Component(
  "WeatherAssistant",
  async ({ prompt }: { prompt: string }) => {
    const result = await generateText({
      messages: [
        {
          role: "system",
          content: "You're a helpful, friendly weather assistant.",
        },
        {
          role: "user",
          content: prompt,
        },
      ],
      model: openai("gpt-4o-mini"),
      tools: { weather: weatherTool },
      maxSteps: 5,
    });
    return result.text;
  },
);
```

### Using tools with streaming

You can also use tools with streaming responses using `streamText`:

```ts
const StreamingWeatherAssistant = gensx.Component(
  "StreamingWeatherAssistant",
  ({ prompt }: { prompt: string }) => {
    const result = streamText({
      messages: [
        {
          role: "system",
          content: "You're a helpful, friendly weather assistant.",
        },
        {
          role: "user",
          content: prompt,
        },
      ],
      model: openai("gpt-4o-mini"),
      tools: { weather: weatherTool },
      maxSteps: 5,
    });

    const generator = async function* () {
      for await (const chunk of result.textStream) {
        yield chunk;
      }
    };

    return generator();
  },
);
```

## Tools with the OpenAI SDK

You can also use the [`@gensx/openai`](../component-reference/openai.mdx) package to work with tools using OpenAI's native tool-calling capabilities.

### Defining a tool

Define your tool:

```ts
const weatherTool = {
  type: "function" as const,
  function: {
    name: "get_weather",
    description: "get the weather for a given location",
    parameters: {
      type: "object",
      properties: {
        location: {
          type: "string",
          description: "The location to get the weather for",
        },
      },
      required: ["location"],
    },
    parse: JSON.parse,
    function: (args: { location: string }) => {
      console.log("getting weather for", args.location);
      const weather = ["sunny", "cloudy", "rainy", "snowy"];
      return {
        weather: weather[Math.floor(Math.random() * weather.length)],
      };
    },
  },
};
```

### Using tools with `runTools`

The OpenAI SDK provides a `runTools` method that handles the tool calling process:

```ts
const WeatherAssistant = gensx.Component(
  "WeatherAssistant",
  async ({ prompt }: { prompt: string }) => {
    const result = await openai.beta.chat.completions.runTools({
      model: "gpt-4.1-mini",
      messages: [
        {
          role: "system",
          content: "You're a helpful, friendly weather assistant.",
        },
        {
          role: "user",
          content: prompt,
        },
      ],
      tools: [weatherTool],
    });
    return await result.finalContent();
  },
);
```

### Using tools with streaming

You can also use tools with streaming responses:

```ts
const StreamingWeatherAssistant = gensx.Component(
  "StreamingWeatherAssistant",
  async ({ prompt }: { prompt: string }) => {
    const result = await openai.beta.chat.completions.runTools({
      model: "gpt-4.1-mini",
      messages: [
        {
          role: "system",
          content: "You're a helpful, friendly weather assistant.",
        },
        {
          role: "user",
          content: prompt,
        },
      ],
      tools: [weatherTool],
      stream: true,
    });
    return result;
  },
);
```

Then to consume the output of the component, you would do the following:

```ts
const streamToolsResult = await StreamingTools({
  prompt,
});
for await (const chunk of streamToolsResult) {
  process.stdout.write(chunk.choices[0].delta.content ?? "");
}
```

## Resources

For more examples of using tools with GenSX, see the following examples:

- [Vercel AI SDK tools example](https://github.com/gensx-inc/gensx/blob/main/examples/vercel-ai)
- [OpenAI tools example](https://github.com/gensx-inc/gensx/blob/main/examples/openai-examples)
