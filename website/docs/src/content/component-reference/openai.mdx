---
title: OpenAI
description: OpenAI API compatible component reference
---

# OpenAI

The [@gensx/openai](https://www.npmjs.com/package/@gensx/openai) package provides OpenAI API compatible components for GenSX.

## Installation

To install the package, run the following command:

```bash
npm install @gensx/openai
```

## Supported components

| <div style={{width: "150px"}}>Component</div>   | Description                                                                                     |
| :---------------------------------------------- | :---------------------------------------------------------------------------------------------- |
| [`OpenAIProvider`](#openaiprovider)             | OpenAI Provider that handles configuration and authentication for child components              |
| [`GSXChatCompletion`](#gsxchatcompletion)       | Enhanced component with enhanced features for OpenAI chat completions                           |
| [`ChatCompletion`](#chatcompletion)             | Simplified component for chat completions with streamlined output interface                     |
| [`OpenAIChatCompletion`](#openaichatcompletion) | Low-level component that directly matches the OpenAI SDK interface for the Chat Completions API |
| [`OpenAIResponses`](#openairesponses)           | Low-level component that directly matches the OpenAI SDK interface for the Responses API        |

## Component Comparison

The package provides three different chat completion components to suit different use cases:

- **OpenAIChatCompletion**: Direct mapping to the OpenAI API with identical inputs and outputs
- **GSXChatCompletion**: Enhanced component with additional features like structured output and automated tool calling
- **ChatCompletion**: Simplified interface that returns string responses or simple streams while maintaining identical inputs to the OpenAI API

## Reference

#### `<OpenAIProvider/>`

The `OpenAIProvider` component initializes and provides an OpenAI client instance to all child components. Any components that use OpenAI's API need to be wrapped in an `OpenAIProvider`.

```tsx
import { OpenAIProvider } from "@gensx/openai";

<OpenAIProvider
  apiKey="your-api-key" // Your OpenAI API key
  organization="org-id" // Optional: Your OpenAI organization ID
  baseURL="https://api.openai.com/v1" // Optional: API base URL
/>;
```

By configuring the baseURL, you can also use the `OpenAIProvider` with other OpenAI compatible APIs like [x.AI](https://docs.x.ai/docs/overview#featured-models) and [Groq](https://console.groq.com/docs/openai).

```tsx
<OpenAIProvider
  apiKey="your-api-key" // Your Groq API key
  baseURL="https://api.groq.com/openai/v1"
/>
```

##### Props

The `OpenAIProvider` accepts all configuration options from the [OpenAI Node.js client library](https://github.com/openai/openai-node) including:

- `apiKey` (required): Your OpenAI API key
- `organization`: Optional organization ID
- `baseURL`: Optional API base URL

#### `<GSXChatCompletion/>`

The `GSXChatCompletion` component is an advanced chat completion component that provides enhanced features beyond the standard OpenAI API. It supports structured output, tool calling, and streaming, with automatic handling of tool execution.

```tsx
import * as gensx from "@gensx/core";
import { GSXChatCompletion, GSXTool } from "@gensx/openai";
import { z } from "zod";

// Example with structured output
const result = await gensx.execute(
  <GSXChatCompletion
    model="gpt-4o"
    messages={[
      { role: "system", content: "You are a helpful assistant." },
      {
        role: "user",
        content: "Extract the name and age from: John Doe, 32 years old",
      },
    ]}
    outputSchema={z.object({
      name: z.string(),
      age: z.number(),
    })}
  />,
);
// result is typed as { name: string, age: number }

// Example with tools
const calculator = GSXTool.create({
  name: "calculator",
  description: "Perform mathematical calculations",
  schema: z.object({
    expression: z.string(),
  }),
  run: async ({ expression }) => {
    return { result: eval(expression) };
  },
});

const toolResult = await gensx.execute(
  <GSXChatCompletion
    model="gpt-4o"
    messages={[{ role: "user", content: "What is 123 * 456?" }]}
    tools={[calculator]}
  />,
);
```

##### Props

The `GSXChatCompletion` component accepts all parameters from OpenAI's chat completion API plus additional options:

- `model` (required): ID of the model to use (e.g., `"gpt-4o"`, `"gpt-4o-mini"`)
- `messages` (required): Array of messages in the conversation
- `stream`: Whether to stream the response (when `true`, returns a `Stream<ChatCompletionChunk>`)
- `tools`: Array of `GSXTool` instances for function calling
- `outputSchema`: Zod schema for structured output (when provided, returns data matching the schema)
- `structuredOutputStrategy`: Strategy to use for structured output. Supported values are `default`, `tools`, and `response_format`.
- Plus all standard OpenAI chat completion parameters (temperature, maxTokens, etc.)

##### Return Types

The return type of `GSXChatCompletion` depends on the props:

- With `stream: true`: Returns `Stream<ChatCompletionChunk>` from OpenAI SDK
- With `outputSchema`: Returns data matching the provided Zod schema
- Default: Returns `GSXChatCompletionResult` (OpenAI response with message history)

#### `<ChatCompletion/>`

The `ChatCompletion` component provides a simplified interface for chat completions. It returns either a string or a simple stream of string tokens while having identical inputs to the OpenAI API.

```tsx
import * as gensx from "@gensx/core";
import { ChatCompletion } from "@gensx/openai";

// Non-streaming usage (returns a string)
const response = await gensx.execute(
  <ChatCompletion
    model="gpt-4o"
    messages={[
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: "What's a programmable tree?" },
    ]}
    temperature={0.7}
  />,
);

// Streaming usage (returns an AsyncIterableIterator<string>)
const stream = await gensx.execute(
  <ChatCompletion
    model="gpt-4o"
    messages={[
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: "What's a programmable tree?" },
    ]}
    temperature={0.7}
    stream={true}
  />,
);
```

##### Props

The `ChatCompletion` component accepts all parameters from OpenAI's chat completion API:

- `model` (required): ID of the model to use (e.g., `"gpt-4o"`, `"gpt-4o-mini"`)
- `messages` (required): Array of messages in the conversation
- `temperature`: Sampling temperature (0-2)
- `stream`: Whether to stream the response
- `maxTokens`: Maximum number of tokens to generate
- `responseFormat`: Format of the response (example: `{ "type": "json_object" }`)
- `tools`: Array of `GSXTool` instances for function calling

##### Return Types

- With `stream: false` (default): Returns a string containing the model's response
- With `stream: true`: Returns an `AsyncIterableIterator<string>` that yields tokens as they're generated

#### `<OpenAIChatCompletion/>`

The `OpenAIChatCompletion` component is a low-level component that directly maps to the OpenAI SDK. It has identical inputs and outputs to the OpenAI API, making it suitable for advanced use cases where you need full control.

```tsx
import * as gensx from "@gensx/core";
import { OpenAIChatCompletion } from "@gensx/openai";

// Non-streaming usage
const completion = await gensx.execute(
  <OpenAIChatCompletion
    model="gpt-4o"
    messages={[
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: "What's a programmable tree?" },
    ]}
  />,
);
console.log(completion.choices[0].message.content);

// Streaming usage
const stream = await gensx.execute(
  <OpenAIChatCompletion
    model="gpt-4o"
    messages={[
      { role: "system", content: "You are a helpful assistant." },
      { role: "user", content: "What's a programmable tree?" },
    ]}
    stream={true}
  />,
);

for await (const chunk of stream) {
  console.log(chunk.choices[0]?.delta?.content || "");
}
```

##### Props

The `OpenAIChatCompletion` component accepts all parameters from the OpenAI SDK's `chat.completions.create` method:

- `model` (required): ID of the model to use
- `messages` (required): Array of messages in the conversation
- `temperature`: Sampling temperature
- `stream`: Whether to stream the response
- `maxTokens`: Maximum number of tokens to generate
- `tools`: Array of OpenAI tool definitions for function calling
- Plus all other OpenAI chat completion parameters

##### Return Types

- With `stream: false` (default): Returns the full `ChatCompletionOutput` object from OpenAI SDK
- With `stream: true`: Returns a `Stream<ChatCompletionChunk>` from OpenAI SDK

#### `<OpenAIResponses/>`

The `OpenAIResponses` component is a low-level component that directly maps to the [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses). It has identical inputs and outputs to the OpenAI Responses API, making it suitable for advanced use cases where you need full control.

```tsx
import * as gensx from "@gensx/core";
import { OpenAIResponses, OpenAIProvider } from "@gensx/openai";

const response = await gensx.execute(
  <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
    <OpenAIResponses
      model="gpt-4o-mini"
      input={[
        {
          role: "user",
          content: "What is JSX?",
        },
      ]}
      truncation="auto"
    />
  </OpenAIProvider>,
);

console.log(response.output_text);
```

##### Props

The `OpenAIResponses` component accepts all parameters from the OpenAI SDK's `responses.create` method:

- `model` (required): ID of the model to use
- `input` (required): The input to the response
- Plus all other optional parameters from the OpenAI Responses API

##### Return Types

- With `stream: false` (default): Returns the full `Response` object from OpenAI SDK
- With `stream: true`: Returns a `Stream<ResponseStreamEvent>` from OpenAI SDK
