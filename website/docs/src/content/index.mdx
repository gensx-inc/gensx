---
title: GenSX Overview
description: GenSX overview
sidebarTitle: Overview
---

# GenSX Overview

GenSX is a simple TypeScript framework for building complex LLM applications. It‚Äôs a workflow engine designed for building agents, chatbot APIs, and more using common patterns like RAG and reflection. In addition, GenSX Cloud offers serverless hosting, cloud storage, and tracing and observability to build production-ready agents and workflows.

It uses `jsx` to define and orchestrate workflows with functional, reusable components:

{/* prettier-ignore-start */}

```tsx
const WriteBlog = gensx.Component<WriteBlogProps, string>(
  "WriteBlog",
  async ({ prompt }) => {
    return (
      <WriteDraft prompt={prompt}>
        {(blog) => (
          <EditDraft draft={blog}>
        {(draft) => console.log(draft)}
      </WriteDraft>
    )}
  </WriteBlog>,
);

const workflow = gensx.Workflow("WriteBlogWorkflow", WriteBlog);
const result = await workflow.run({ prompt: "Write a blog post about AI developer tools" });
```

{/* prettier-ignore-end */}

Most LLM frameworks are graph oriented, inspired by popular python tools like Airflow. You express nodes, edges, and a global state object for your workflow. While graph APIs are highly expressive, they are also cumbersome:

- Building a mental model and visualizing the execution of a workflow from a node/edge builder is difficult.
- Global state makes refactoring difficult.
- All of this leads to low velocity when experimenting with and evolving your LLM workflows.

LLM workflows are fundamentally about function composition and data flow - exactly what JSX was designed to express. There is no need for a graph DSL. GenSX expresses control flow, loops, and recursion using plain old typescript language primitives. To learn more about why GenSX uses JSX, read [Why JSX?](docs/why-jsx).

While GenSX uses JSX, it does not share the same execution model as React, and has zero dependencies.

## GenSX Cloud

[GenSX Cloud](/docs/cloud) provides everything you need to ship production grade agents and workflow including a serverless runtime designed for long-running workloads, cloud storage to build stateful workflows and agents, and tracing and observability.

### Serverless deployments

Deploy any workflow to a hosted API endpoint with a single command. The GenSX cloud platform handles scaling, infrastructure management, and API generation automatically:

```bash
# Deploy your workflow to GenSX Cloud
$ gensx deploy ./src/workflows.tsx
```

```bash
‚úî Building workflow using Docker
‚úî Generating schema

‚úî Successfully built project

‚Ñπ Using project name from gensx.yaml: support-tools
‚úî Deploying project to GenSX Cloud (Project: support-tools)

‚úî Successfully deployed project to GenSX Cloud

Dashboard: https://app.gensx.com/gensx/support-tools/default/workflows

Available workflows:
- ChatAgent
- TextToSQLWorkflow
- RAGWorkflow
- AnalyzeDiscordWorkflow

Project: support-tools
Environment: default
```

The platform is optimized for AI workloads with millisecond-level cold starts and support for long-running executions up to an hour.

### Tracing and observability

GenSX Cloud provides comprehensive tracing and observability for all your workflows and agents.

![Workflow component tree](/cloud/trace.png)

Inputs and outputs are recorded for every component that executes in your workflows, including prompts, tool calls, and token usage. This makes it easy to debug hallucinations, prompt upgrades, and monitor costs.

![Component trace](/cloud/component-trace.png)

### Cloud Storage

Build stateful AI applications with zero configuration using built-in storage primitives that provide managed blob storage, SQL databases, and full-text + vector search:

```tsx
const ChatWithMemory = gensx.Component<ChatWithMemoryProps, string>(
  "ChatWithMemory",
  async (props) => {
    const { userInput, threadId } = props;

    // Load chat history from blob storage
    const blob = useBlob<ChatMessage[]>(`chat-history/${threadId}.json`);
    const history = await blob.getJSON();

    // Add new message and run LLM
    // ...

    // Save updated history
    await blob.putJSON(updatedHistory);

    return response;
  },
);
```

For more details about GenSX Cloud see the [complete cloud reference](/docs/cloud).

## Reusable by default

GenSX components are pure functions, depend on zero global state, and are _reusable_ by default. Components accept `props` and return an output.

```tsx
interface ResearchTopicProps {
  topic: string;
}
type ResearchTopicOutput = string;
const ResearchTopic = gensx.Component<ResearchTopicProps, ResearchTopicOutput>(
  "ResearchTopic",
  async ({ topic }) => {
    console.log("üìö Researching topic:", topic);
    const systemPrompt = `You are a helpful assistant that researches topics...`;

    return (
      <ChatCompletion
        model="gpt-4o-mini"
        temperature={0}
        messages={[
          { role: "system", content: systemPrompt },
          { role: "user", content: topic },
        ]}
      />
    );
  },
);
```

Because components are pure functions, they are easy to test and eval in isolation. This enables you to move quickly and experiment with the structure of your workflow.

A second order benefit of reusability is that components can be _shared_ and published to package managers like `npm`. If you build a functional component, you can make it available to the community - something that frameworks that depend on global state preclude by default.

## Composition

All GenSX components support nesting, a pattern to access component outputs via a child function. This creates a natural way to pass data between steps:

```tsx
export const WriteBlog = gensx.StreamComponent<WriteBlogProps>(
  "WriteBlog",
  async ({ prompt }) => {
    return (
      <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
        <Research prompt={prompt}>
          {(research) => (
            <WriteDraft prompt={prompt} research={research.flat()}>
              {(draft) => <EditDraft draft={draft} stream={true} />}
            </WriteDraft>
          )}
        </Research>
      </OpenAIProvider>
    );
  },
);
```

There is no need for a DSL or graph API to define the structure of your workflow. More complex patterns like cycles and agents can be encapsulated in components that use standard loops and conditionals. Typescript and JSX unifies workflow definition and execution in plain old typescript, with the ability to express all of the same patterns.

## Visual clarity

Workflow composition with JSX reads naturally from top to bottom like a standard programming language.

```tsx
<FetchHNPosts limit={postCount}>
  {(stories) => (
    <AnalyzeHNPosts stories={stories}>
      {({ analyses }) => (
        <GenerateReport analyses={analyses}>
          {(report) => (
            <EditReport content={report}>
              {(editedReport) => (
                <WriteTweet
                  context={editedReport}
                  prompt="Summarize the HN trends in a tweet"
                />
              )}
            </EditReport>
          )}
        </GenerateReport>
      )}
    </AnalyzeHNPosts>
  )}
</FetchHNPosts>
```

Contrast this with graph APIs, where you need to build a mental model of the workflow from a node/edge builder:

```tsx
const graph = new Graph()
  .addNode("fetchHNPosts", fetchHNPosts)
  .addNode("analyzeHNPosts", analyzePosts)
  .addNode("generateReport", generateReport)
  .addNode("editReport", editReport)
  .addNode("writeTweet", writeTweet);

graph
  .addEdge(START, "fetchHNPosts")
  .addEdge("fetchHNPosts", "analyzeHNPosts")
  .addEdge("analyzeHNPosts", "generateReport")
  .addEdge("generateReport", "editReport")
  .addEdge("editReport", "writeTweet")
  .addEdge("writeTweet", END);
```

Nesting makes dependencies explicit, and it is easy to see the data flow between steps. No graph DSL required.

## Streaming baked in

LLMs are unique in that any given call can return a stream or a prompt response, but streaming is typically applied to just the last step. You shouldn't have to touch the innards of your components to experiment with the shape of your workflow.

GenSX makes this easy to handle with `StreamComponent`.

A single component implementation can be used in both streaming and non-streaming contexts by setting the `stream` prop:

```tsx
const EditDraft = gensx.StreamComponent<EditDraftProps>(
  "EditDraft",
  async ({ draft }) => {
    console.log("üîç Editing draft");
    const systemPrompt = `You are a helpful assistant that edits blog posts...`;

    return (
      <ChatCompletion
        stream={true}
        model="gpt-4o-mini"
        temperature={0}
        messages={[
          { role: "system", content: systemPrompt },
          { role: "user", content: draft },
        ]}
      />
    );
  },
);
```

From there you can use the component in a streaming context:

```tsx
const stream = await gensx
  .Workflow("EditDraftWorkflow", EditDraft)
  .run({ draft, stream: true });

for await (const chunk of stream) {
  process.stdout.write(chunk);
}
```

And you can use the component in a non-streaming context:

```tsx
const result = await gensx
  .Workflow("EditDraftWorkflow", EditDraft)
  .run({ draft, stream: false });

console.log(result);
```

## Designed for velocity

The GenSX programming model is optimized for speed of iteration in the long run.

The typical journey building an LLM application looks like:

1. Ship a prototype that uses a single LLM prompt.
2. Add in evals to measure progress.
3. Add in external context via RAG.
4. Break tasks down into smaller discrete LLM calls chained together to improve quality.
5. Add advanced patterns like reflection, and agents.

Experimentation speed depends on the ability to refactor, rearrange, and inject new steps. In our experience, this is something that frameworks that revolve around global state and a graph model slow down.

The functional component model in GenSX support an iterative loop that is fast on day one through day 1000.
