---
title: Search
description: Full-text and vector search for GenSX workflows
---

# Search

GenSX's Cloud search service provides full-text and vector search for AI applications. It enables you to store, query, and manage vector embeddings for semantic search, retrieval-augmented generation (RAG), and other AI use cases.

Search is powered by [turbopuffer](https://turbopuffer.com/), fully featured and ready for AI workloads:

- **Combined vector and keyword search**: Perform hybrid searches using both semantic similarity (vectors) and keyword matching (BM25).
- **Millisecond query latency**: Get results quickly, even with large vector collections.
- **Flexible filtering**: Apply metadata filters to narrow down search results based on categories, timestamps, or any custom attributes.

## Basic usage

To use search in your GenSX application:

1. Install the storage package:

   ```bash
   npm install @gensx/storage
   ```

2. Access search namespaces within your components using the `useSearch` hook:

   ```ts
   import { useSearch } from "@gensx/storage";

   const search = await useSearch("documents");
   ```

### Storing vector embeddings

The first step in using search is to convert your data into vector embeddings and store them:

```ts
import * as gensx from "@gensx/core";
import { useSearch } from "@gensx/storage";
import { embed, embedMany, generateText } from "@gensx/vercel-ai";
import { openai } from "@ai-sdk/openai";

const IndexDocuments = gensx.Component(
  "IndexDocuments",
  async ({ documents }) => {
    // Get access to a search namespace
    const search = await useSearch("documents");

    // Generate embeddings for the documents
    const { embeddings } = await embedMany({
      model: openai.embedding("text-embedding-3-small"),
      values: documents.map((doc) => doc.text),
    });

    // Store the embeddings with original text as metadata
    await search.write({
      upsertRows: documents.map((doc, index) => ({
        id: doc.id,
        vector: embeddings[index],
        text: doc.text,
        category: doc.category,
        createdAt: new Date().toISOString(),
      })),
      distanceMetric: "cosine_distance",
    });

    return { success: true, count: documents.length };
  },
);
```

### Searching for similar documents

Once you've stored embeddings, you can search for semantically similar content:

```ts
const SearchDocuments = gensx.Component(
  "SearchDocuments",
  async ({ query, category }: SearchDocumentsInput) => {
    // Get access to the search namespace
    const search = await useSearch("documents");

    // Generate an embedding for the query
    const { embedding } = await embed({
      model: openai.embedding("text-embedding-3-small"),
      value: query,
    });

    // Build query options
    const queryOptions = {
      rankBy: ["vector", "ANN", embedding] as const,
      includeAttributes: true,
      topK: 5, // Return top 5 results
    };

    // Add filters if category is specified
    if (category) {
      queryOptions.filters = ["category", "Eq", category];
    }

    // Perform the search
    const results = await search.query(queryOptions);

    // Process and return results from the rows array
    return results.rows?.map((result) => ({
      id: result.id,
      text: result.text,
      distance: result.$dist,
    })) || [];
  },
);
```

## Building a RAG application

Retrieval-Augmented Generation (RAG) is one of the most common use cases for vector search. Here's how to build a complete RAG workflow:

### Step 1: Index your documents

First, create a component to prepare and index your documents:

```ts
const PrepareDocuments = gensx.Component("PrepareDocuments", async () => {
  // Sample baseball player data
  const documents = [
    {
      id: "1",
      text: "Marcus Bennett is a first baseman for the Portland Pioneers. He has 32 home runs this season.",
      category: "player",
    },
    {
      id: "2",
      text: "Ethan Carter plays shortstop for the San Antonio Stallions with 24 home runs.",
      category: "player",
    },
    {
      id: "3",
      text: "The Portland Pioneers are leading the Western Division with a 92-70 record.",
      category: "team",
    },
  ];

  // Index the documents
  return await IndexDocuments({ documents });
});
```

### Step 2: Create a query tool

Next, create a tool that can access your search index:

```ts
import { tool } from "ai";
import { z } from "zod";

// Define a tool to query the search index
const queryTool = tool({
  description: "Query the baseball knowledge base",
  parameters: z.object({
    query: z.string().describe("The text query to search for"),
  }),
  execute: async ({ query }) => {
    // Access search index
    const search = await useSearch("baseball");

    // Generate query embedding
    const { embedding } = await embed({
      model: openai.embedding("text-embedding-3-small"),
      value: query,
    });

    // Search for relevant documents
    const results = await search.query({
      rankBy: ["vector", "ANN", embedding],
      includeAttributes: true,
      topK: 10,
    });

    // Return formatted results
    return JSON.stringify(
      results.rows?.map((r) => r.text) || [],
      null,
      2,
    );
  },
});
```

### Step 3: Create the RAG agent

Now, create an agent that uses the query tool to access relevant information:

```ts
const RagAgent = gensx.Component("RagAgent", ({ question }) => {
  return generateText({
    messages: [
      {
        role: "system",
        content: `You are a baseball expert assistant. Use the query tool to
          look up relevant information before answering questions.`,
      },
      { role: "user", content: question },
    ],
    model: openai("gpt-4.1-mini"),
    tools: { query: queryTool },
    maxSteps: 5,
  });
});
```

### Step 4: Combine Everything in a Workflow

Finally, put it all together in a workflow:

```ts
const RagWorkflow = gensx.Component(
  "RagWorkflow",
  async ({ question, shouldReindex }: RagWorkflowInput) => {
    // Optionally reindex documents
    if (shouldReindex) {
      await PrepareDocuments();
    }

    // Use the RAG agent to answer the question
    return await RagAgent({ question });
  },
);
```

## Practical examples

### Agent memory system

One powerful application of vector search is creating a long-term memory system for AI agents:

```ts
import * as gensx from "@gensx/core";
import { useSearch } from "@gensx/storage";
import { embed, embedMany, generateText } from "@gensx/vercel-ai";
import { openai } from "@ai-sdk/openai";

// Component to store a memory
const StoreMemory = gensx.Component(
  "StoreMemory",
  async ({ userId, memory, importance = "medium" }) => {
    const search = await useSearch(`memories-${userId}`);

    // Generate embedding for this memory
    const { embedding } = await embed({
      model: openai.embedding("text-embedding-3-small"),
      value: memory,
    });

    // Store the memory with metadata
    await search.write({
      upsertRows: [
        {
          id: `memory-${Date.now()}`,
          vector: embedding,
          content: memory,
          timestamp: new Date().toISOString(),
          importance: importance, // "high", "medium", "low"
          source: "user-interaction",
        },
      ],
      distanceMetric: "cosine_distance",
    });

    return { success: true };
  },
);

// Component to recall relevant memories
const RecallMemories = gensx.Component(
  "RecallMemories",
  async ({ userId, context, maxResults = 5 }) => {
    const search = await useSearch(`memories-${userId}`);

    // Generate embedding for the context
    const { embedding } = await embed({
      model: openai.embedding("text-embedding-3-small"),
      value: context,
    });

    // Query for relevant memories, prioritizing important ones
    const results = await search.query({
      rankBy: ["vector", "ANN", embedding],
      topK: maxResults,
      includeAttributes: true,
    });

    // Format memories for the agent from the rows array
    return results.rows?.map((result) => ({
      content: result.content,
      timestamp: result.timestamp,
      distance: result.$dist?.toFixed(3),
    })) || [];
  },
);

// Component that uses memories in a conversation
const MemoryAwareAgent = gensx.Component(
  "MemoryAwareAgent",
  async ({ userId, userMessage }) => {
    // Recall relevant memories based on the current conversation
    const memories = await RecallMemories({
      userId,
      context: userMessage,
      maxResults: 3,
    });

    // Use memories to inform the response
    const response = await generateText({
      messages: [
        {
          role: "system",
          content: `You are an assistant with memory. Consider these relevant memories about this user:
          ${memories.map((m) => `[${m.timestamp}] ${m.content} (distance: ${m.distance})`).join("\n")}`,
        },
        { role: "user", content: userMessage },
      ],
      model: openai("gpt-4.1-mini"),
    });

    // Store this interaction as a new memory
    await StoreMemory({
      userId,
      memory: `User asked: "${userMessage}". I replied: "${response.text}"`,
      importance: "medium",
    });

    return response.text;
  },
);
```

### Knowledge base search

Another powerful application is a knowledge base with faceted search capabilities:

```ts
const SearchKnowledgeBase = gensx.Component(
  "SearchKnowledgeBase",
  async ({ query, filters = {} }) => {
    const search = await useSearch("knowledge-base");

    // Generate embedding for the query
    const { embedding } = await embed({
      model: openai.embedding("text-embedding-3-small"),
      value: query,
    });

    // Build filter conditions from user-provided filters
    let filterConditions = ["And", []];

    if (filters.category) {
      filterConditions[1].push(["category", "Eq", filters.category]);
    }

    if (filters.dateRange) {
      filterConditions[1].push([
        "publishedDate",
        "Gte",
        filters.dateRange.start,
      ]);
      filterConditions[1].push(["publishedDate", "Lte", filters.dateRange.end]);
    }

    if (filters.tags && filters.tags.length > 0) {
      filterConditions[1].push(["tags", "ContainsAny", filters.tags]);
    }

    // Perform hybrid search (vector + keyword) with filters
    const results = await search.query({
      rankBy: ["text", "BM25", query], // Text-based ranking for hybrid search
      includeAttributes: true,
      topK: 10,
      filters: filterConditions[1].length > 0 ? filterConditions : undefined,
    });

    // Return formatted results from the rows array
    return results.rows?.map((result) => ({
      title: result.title,
      snippet: result.snippet,
      url: result.url,
      category: result.category,
      tags: result.tags,
      score: result.$dist,
    })) || [];
  },
);
```

## Advanced usage

### Filtering by metadata

Use filters to narrow down search results:

```ts
const search = await useSearch("articles");

// Search with filters
const results = await search.query({
  rankBy: ["vector", "ANN", queryEmbedding],
  topK: 10,
  filters: [
    "And",
    [
      ["category", "Eq", "sports"],
      ["publishDate", "Gte", "2023-01-01"],
      ["publishDate", "Lt", "2024-01-01"],
      ["author", "In", ["Alice", "Bob", "Carol"]],
    ],
  ],
});
```

### Updating schema

Manage your vector collection's schema:

```ts
const search = await useSearch("products");

// Get current schema
const currentSchema = await search.getSchema();

// Update schema to add new fields
await search.updateSchema({
  schema: {
    ...currentSchema,
    newField: { type: "int" },
    anotherField: { type: "[]string" },
  },
});
```

## Reference

See the [search component reference](docs/component-reference/storage-components/search-reference) for full details.