---
title: Quickstart
description: Getting started with GenSX
---

# Quickstart

In this quickstart, you'll learn how to get up and running with GenSX. GenSX is a simple typescript framework for building complex LLM applications using JSX. If you haven't already, check out the [basic concepts](/docs/basic-concepts) to learn more about how GenSX works.

## Prerequisites

Before getting started, make sure you have the following:

- [Node.js](https://nodejs.org/) version 20 or higher installed
- An [OpenAI API key](https://platform.openai.com/api-keys) with access to the required models
- A package manager of your choice ([npm](https://www.npmjs.com/), [yarn](https://yarnpkg.com/), or [pnpm](https://pnpm.io/))

## Install the `gensx` CLI

You can install the `gensx` CLI using your package manager of choice:

```bash
# For a global installation
npm i -g gensx

# or prefix every command with npx
npx gensx
```

## Login to GenSX Cloud (optional)

If you want to be able to visualize your workflows and view traces, you'll need to login to GenSX Cloud:

![blog writing trace](/quickstart/blog-trace.png)

Run the following command to log into GenSX Cloud:

```bash
# If you installed the CLI globally
gensx login

# Using npx
npx gensx login
```

Now traces will automatically be saved to the cloud so you can visualize and debug workflow executions, but this step is optional and you can skip it for now if you prefer.

![gensx login](/quickstart/gensx-login.png)

Hit enter and your browser will open the login page.

![console login](/quickstart/login.png)

Once you've logged in, you'll see the following success message:

![login success](/quickstart/login-success.png)

Now you're ready to create a new workflow! Return to your terminal for the next steps.

## Create a new workflow

To get started, run the following command with your package manager of choice in an empty directory. This will create a new GenSX workflow to get you started.

```bash
# If you installed the CLI globally
gensx new .

# Using npx
npx gensx new .
```

When creating a new project, you'll be prompted to select IDE rules to add to your project. These rules help AI assistants like Claude, Cursor, Cline, and Windsurf understand your GenSX project better, providing more accurate code suggestions and help.

You can also specify IDE rules directly using the `--ide-rules` flag:

```bash
# Install Cline and Windsurf rules
gensx new . --ide-rules cline,windsurf

# Skip IDE rule selection
gensx new . --skip-ide-rules
```

You can also install or update these rules later using npx:

```bash
# Install Claude.md template
npx @gensx/claude-md

# Install Cursor rules
npx @gensx/cursor-rules

# Install Cline rules
npx @gensx/cline-rules

# Install Windsurf rules
npx @gensx/windsurf-rules
```

This will create a new started project, ready for you to run your first workflow:

![create project](/quickstart/gensx-new.png)

In `index.tsx`, you'll find a simple OpenAI chat completion component:

```tsx
import * as gensx from "@gensx/core";
import { OpenAIProvider, ChatCompletion } from "@gensx/openai";

interface RespondProps {
  userInput: string;
}
type RespondOutput = string;

const Respond = gensx.Component<RespondProps, RespondOutput>(
  "Respond",
  async ({ userInput }) => {
    return (
      <ChatCompletion
        model="gpt-4o-mini"
        messages={[
          {
            role: "system",
            content:
              "You are a helpful assistant. Respond to the user's input.",
          },
          { role: "user", content: userInput },
        ]}
      />
    );
  },
);

const WorkflowComponent = gensx.Component<{ userInput: string }, string>(
  "Workflow",
  ({ userInput }) => (
    <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
      <Respond userInput={userInput} />
    </OpenAIProvider>
  ),
);

const workflow = gensx.Workflow("MyGSXWorkflow", WorkflowComponent);

const result = await workflow.run({
  userInput: "Hi there! Say 'Hello, World!' and nothing else.",
});

console.log(result);
```

The component is executed through `gensx.Workflow.run()`, which processes the JSX tree from top to bottom. In this example:

1. First, the `OpenAIProvider` component is initialized with your API key
2. Then, the `Respond` component receives the `userInput` prop
3. Inside `Respond`, a `ChatCompletion` component is created with the specified model and messages
4. The result flows back up through the tree, ultimately returning the response from gpt-4o-mini.

Components in GenSX are pure functions that take props and return outputs, making them easy to test and compose. The JSX structure makes the data flow clear and explicit - each component's output can be used by its children through standard TypeScript/JavaScript.

### Running the workflow

To run the workflow, you'll need to set the `OPENAI_API_KEY` environment variable.

```bash
# Set the environment variable
export OPENAI_API_KEY=<your-api-key>

# Run the project
pnpm dev
```

![run project](/quickstart/gensx-run.png)

If you chose to log in, you can now view the trace for this run in GenSX Cloud by clicking the link:

![trace](/quickstart/trace.png)

The trace shows a flame graph of your workflow, including every component that executed with inputs and outputs.

Some components will be hidden by default, but you can click the carat to expand them. Clicking on a component will show you details about it's input props and outputs.

For longer running workflows, this view will update in real time as the workflow executes.

## Combining components

The example above is a simple workflow with a single component. In practice, you'll often want to combine multiple components to create more complex workflows.

Components can be nested to create multi-step workflows with each component's output being passed through a child function. For example, let's define two components: a `Research` component that gathers information about a topic, and a `Writer` component that uses that information to write a blog post.

```tsx
// Research component that gathers information
const Research = gensx.Component<{ topic: string }, string>(
  "Research",
  async ({ topic }) => {
    return (
      <ChatCompletion
        model="gpt-4o-mini"
        messages={[
          {
            role: "system",
            content:
              "You are a research assistant. Provide key facts about the topic.",
          },
          { role: "user", content: topic },
        ]}
      />
    );
  },
);

// Writer component that uses research to write content
const WriteArticle = gensx.Component<
  { topic: string; research: string },
  string
>("WriteArticle", async ({ topic, research }) => {
  return (
    <ChatCompletion
      model="gpt-4o-mini"
      messages={[
        {
          role: "system",
          content:
            "You are a content writer. Use the research provided to write a blog post about the topic.",
        },
        { role: "user", content: `Topic: ${topic}\nResearch: ${research}` },
      ]}
    />
  );
});
```

Now you can combine these components using a child function:

```tsx
// Combine components using child functions
const ResearchAndWrite = gensx.Component<{ topic: string }, string>(
  "ResearchAndWrite",
  ({ topic }) => (
    <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
      <Research topic={topic}>
        {(research) => <WriteArticle topic={topic} research={research} />}
      </Research>
    </OpenAIProvider>
  ),
);

const workflow = gensx.Workflow("ResearchAndWriteWorkflow", ResearchAndWrite, {
  printUrl: true,
});
```

In this example, the `Research` component gathers information about the topic which then passes the information to the `WriteArticle` component. The `WriteArticle` component uses that information to write an article about the topic which is then returned as the `result`.

### Streaming

One common challenge with LLM workflows is handling streaming responses. Any given LLM call can return a response as a string or as a stream of tokens. Typically you'll want the last component of your workflow to stream the response.

To take advantage of streaming, all you need to do is update the `WriteArticle` component to use `StreamComponent` and set the `stream` prop to `true` in the `ChatCompletion` component.

```tsx
interface WriterProps {
  topic: string;
  research: string;
  stream?: boolean;
}

const Writer = gensx.StreamComponent<WriterProps>(
  "Writer",
  async ({ topic, research, stream }) => {
    return (
      <ChatCompletion
        stream={stream ?? false}
        model="gpt-4o-mini"
        messages={[
          {
            role: "system",
            content:
              "You are a content writer. Use the research provided to write a blog post about the topic.",
          },
          { role: "user", content: `Topic: ${topic}\nResearch: ${research}` },
        ]}
      />
    );
  },
);

const ResearchAndWrite = gensx.Component<{ topic: string }, string>(
  "ResearchAndWrite",
  ({ topic }) => (
    <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
      <Research topic={topic}>
        {(research) => (
          <Writer
            topic="latest quantum computing chips"
            research={research}
            stream={true}
          />
        )}
      </Research>
    </OpenAIProvider>
  ),
);

const workflow = gensx.Workflow("ResearchAndWriteWorkflow", ResearchAndWrite);
const stream = await workflow.run(
  { topic: "latest quantum computing chips" },
  { printUrl: true },
);

// Print the streaming response
for await (const chunk of stream) {
  process.stdout.write(chunk);
}
```

Now we can see our research step running before the write step:

![streaming](/quickstart/writer-trace.png)

While this is nice, the real power of streaming components comes when you expand or refactor your workflow. Now you could easily add an `<EditArticle>` component to the workflow that streams the response to the user with minimal changes. There's no extra plumbing needed besides removing the `stream={true}` prop on the `WriteArticle` component.

```tsx
const ResearchAndWrite = gensx.Component<{ topic: string }, string>(
  "ResearchAndWrite",
  ({ topic }) => (
    <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
      <Research topic={topic}>
        {(research) => (
        <WriteArticle topic={topic} research={research}>
          {(content) => <EditArticle content={content} stream={true}/>}
        </WriteArticle>
      )}
    </Research>
  </OpenAIProvider>,
);

const workflow = gensx.Workflow("ResearchAndWriteWorkflow", ResearchAndWrite, { printUrl: true });
const stream = await workflow.run({ topic: "latest quantum computing chips" });
```

![blog writing trace](/quickstart/blog-trace.png)

## Next steps

Now that you've gone through the quickstart, you should be able to start building with GenSX. You can explore more advanced features and patterns:

### Deploy to GenSX Cloud

GenSX Cloud provides serverless deployment with zero configuration. Deploy your workflow with a single command:

```bash
gensx deploy src/index.ts
```

This will create a REST API endpoint for your workflow that you can call from any application. The endpoint supports both synchronous and asynchronous execution, as well as streaming.

### Add Persistence with Cloud Storage

GenSX Cloud provides built-in storage solutions that require zero configuration:

#### Blob Storage

Use the `useBlob` hook to store and retrieve unstructured data:

```tsx
import { BlobProvider, useBlob } from "@gensx/storage";

const ChatWithMemory = gensx.Component<ChatWithMemoryProps, string>(
  "ChatWithMemory",
  async ({ userInput, threadId }) => {
    // Load chat history from cloud storage
    const blob = useBlob<ChatMessage[]>(`chat-history/${threadId}.json`);
    const history = await blob.getJSON() ?? [];

    // Add user message and get AI response
    // ...

    // Save updated history
    await blob.putJSON(updatedHistory);

    return response;
  }
);

// Wrap your workflow with BlobProvider
const WorkflowComponent = gensx.Component<
  { userInput: string; threadId: string },
  string
>("Workflow", ({ userInput, threadId }) => (
  <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
    <BlobProvider>
      <ChatWithMemory userInput={userInput} threadId={threadId} />
    </BlobProvider>
  </OpenAIProvider>
));
```

#### Database

Use the `useDatabase` hook to work with SQL databases:

```tsx
import { DatabaseProvider, useDatabase } from "@gensx/storage";

const SqlCopilot = gensx.Component<{ question: string }, string>(
  "SqlCopilot",
  async ({ question }) => {
    const db = await useDatabase("baseball");
    const result = await db.execute("SELECT * FROM baseball_stats LIMIT 5");

    // Use the result to inform your LLM response
    // ...
  }
);

// Wrap your workflow with DatabaseProvider
const WorkflowComponent = gensx.Component<{ question: string }, string>(
  "DatabaseWorkflowComponent",
  ({ question }) => (
    <DatabaseProvider kind="cloud">
      <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
        <SqlCopilot question={question} />
      </OpenAIProvider>
    </DatabaseProvider>
  )
);
```

### Run a Local Development Server

Test your workflows locally with a development server that mirrors the cloud environment:

```bash
gensx start src/index.ts
```

This creates API endpoints for each of your workflows that you can call for testing and debugging.

### Explore Example Projects

Take a look at these examples to see how you can build more complex applications:

- [Blog Writer](https://github.com/gensx-inc/gensx/tree/main/examples/blogWriter)
- [Hacker News Analyzer](https://github.com/gensx-inc/gensx/tree/main/examples/hackerNewsAnalyzer)
- [Reflection](https://github.com/gensx-inc/gensx/tree/main/examples/reflection)
- [Chat with Memory](https://github.com/gensx-inc/gensx/tree/main/examples/chat-memory)
- [Text to SQL](https://github.com/gensx-inc/gensx/tree/main/examples/text-to-sql)
