---
title: Quickstart
description: Getting started with GenSX
---

# Quickstart

In this quickstart, you'll learn how to get up and running with GenSX. GenSX is a simple typescript framework for building complex LLM applications using JSX. If you haven't already, check out the [basic concepts](/docs/basic-concepts) to learn more about how GenSX works.

## Prerequisites

Before getting started, make sure you have the following:

- [Node.js](https://nodejs.org/) version 20 or higher installed
- An [OpenAI API key](https://platform.openai.com/api-keys) with access to the required models
- A package manager of your choice ([npm](https://www.npmjs.com/), [yarn](https://yarnpkg.com/), or [pnpm](https://pnpm.io/))

## Install the `gensx` CLI

You can install the `gensx` CLI using your package manager of choice:

```bash
npm i -g gensx
```

Alternatively, if you prefer not to install the CLI globally, you can prefix every command in this guide with `npx`.

## Log in to GenSX Cloud (optional)

If you want to be able to visualize your workflows and view traces, you'll need to log in to GenSX Cloud. This is optional, but recommended.

![blog writing trace](/quickstart/blog-trace.png)

To log in into GenSX Cloud, run the following command:

```bash
gensx login
```

You'll be redirected to the GenSX website and will need to create an account if you don't have one already.

Once you're logged in, you're ready to create a workflow! Workflow traces will automatically be saved to the cloud so you can visualize and debug workflow executions.

## Create a new project

To get started, run the `new` command with a project name of your choice. This will create a new GenSX project with a simple workflow to get you started.

```bash
gensx new <project-name>
```

When creating a new project, you'll be prompted to select IDE rules to add to your project. These rules help AI assistants like Claude, Cursor, Cline, and Windsurf understand your GenSX project better, providing more accurate code suggestions and help.

In `src/workflows.ts`, you'll find a simple `Chat` component and workflow:

```ts
import * as gensx from "@gensx/core";
import { openai } from "@ai-sdk/openai";
import { generateText } from "@gensx/vercel-ai";

interface ChatProps {
  userMessage: string;
}

const Chat = gensx.Component("Chat", async ({ userMessage }: ChatProps) => {
  const result = await generateText({
    model: openai("gpt-4.1-mini"),
    messages: [
      {
        role: "system",
        content: "You are a helpful assistant.",
      },
      { role: "user", content: userMessage },
    ],
  });
  return result.text;
});

const ChatWorkflow = gensx.Workflow(
  "ChatWorkflow",
  async ({ userMessage }: ChatProps) => {
    return await Chat({ userMessage });
  },
);

export { ChatWorkflow };
```

This template shows the basics of building a GenSX workflow:

- Components and workflows are just pure functions that take inputs and return outputs
- You create components and workflows by calling `gensx.Component()` and `gensx.Workflow()` along with a name and a function
- You can use the LLM package of your choice. GenSX provides `@gensx/vercel-ai`, `@gensx/openai`, and `@gensx/anthropic` out of the box. These packages are simply wrappers around the original packages optimized for GenSX.

### Running the workflow

To run the workflow, you simply call it like a function:

```ts
const result = await ChatWorkflow({ userMessage: "Hello, how are you?" });
```

The project template includes a `src/index.ts` file that you can use to run the workflow:

```ts
import { ChatWorkflow } from "./workflows.js";

const result = await ChatWorkflow({
  userMessage: "Hi there! Say 'Hello, World!' and nothing else.",
});

console.log(result);
```

There's nothing special here--workflows are just invoked like any other function.

To run the workflow, you'll need to set the `OPENAI_API_KEY` environment variable.

```bash
# Set the environment variable
export OPENAI_API_KEY=<your-api-key>

# Run the project
pnpm dev
```

This will run the workflow and print the workflow's output to the console along with a URL to the trace (if you're logged in).

```bash
[GenSX] View execution at: https://app.gensx.com/<org>/<project>/executions/<run-id>?workflowName=ChatWorkflow

Hello, World!
```

You can now view the trace for this run in GenSX Cloud by clicking the link:

![trace](/quickstart/trace.png)

The trace shows a flame graph of your workflow, including every component that executed with inputs and outputs.

Some components will be hidden by default, but you can click the carat to expand them. Clicking on a component will show you details about it's inputs and outputs.

For longer running workflows, this view will update in real time as the workflow executes.

## Combining components

The example above is a simple workflow with a single component. In practice, you'll often want to combine multiple components to create more complex workflows.

Components can be nested to create multi-step workflows with each component's output being passed through a child function. For example, let's define two components: a `Research` component that gathers information about a topic, and a `Writer` component that uses that information to write a blog post.

```tsx
// Research component that gathers information
const Research = gensx.Component<{ topic: string }, string>(
  "Research",
  async ({ topic }) => {
    return (
      <ChatCompletion
        model="gpt-4o-mini"
        messages={[
          {
            role: "system",
            content:
              "You are a research assistant. Provide key facts about the topic.",
          },
          { role: "user", content: topic },
        ]}
      />
    );
  },
);

// Writer component that uses research to write content
const WriteArticle = gensx.Component<
  { topic: string; research: string },
  string
>("WriteArticle", async ({ topic, research }) => {
  return (
    <ChatCompletion
      model="gpt-4o-mini"
      messages={[
        {
          role: "system",
          content:
            "You are a content writer. Use the research provided to write a blog post about the topic.",
        },
        { role: "user", content: `Topic: ${topic}\nResearch: ${research}` },
      ]}
    />
  );
});
```

Now you can combine these components using a child function:

```tsx
// Combine components using child functions
const ResearchAndWrite = gensx.Component<{ topic: string }, string>(
  "ResearchAndWrite",
  ({ topic }) => (
    <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
      <Research topic={topic}>
        {(research) => <WriteArticle topic={topic} research={research} />}
      </Research>
    </OpenAIProvider>
  ),
);

const workflow = gensx.Workflow("ResearchAndWriteWorkflow", ResearchAndWrite, {
  printUrl: true,
});
```

In this example, the `Research` component gathers information about the topic which then passes the information to the `WriteArticle` component. The `WriteArticle` component uses that information to write an article about the topic which is then returned as the `result`.

### Streaming

One common challenge with LLM workflows is handling streaming responses. Any given LLM call can return a response as a string or as a stream of tokens. Typically you'll want the last component of your workflow to stream the response.

To take advantage of streaming, all you need to do is update the `WriteArticle` component to use `StreamComponent` and set the `stream` prop to `true` in the `ChatCompletion` component.

```tsx
interface WriterProps {
  topic: string;
  research: string;
  stream?: boolean;
}

const Writer = gensx.StreamComponent<WriterProps>(
  "Writer",
  async ({ topic, research, stream }) => {
    return (
      <ChatCompletion
        stream={stream ?? false}
        model="gpt-4o-mini"
        messages={[
          {
            role: "system",
            content:
              "You are a content writer. Use the research provided to write a blog post about the topic.",
          },
          { role: "user", content: `Topic: ${topic}\nResearch: ${research}` },
        ]}
      />
    );
  },
);

const ResearchAndWrite = gensx.Component<{ topic: string }, string>(
  "ResearchAndWrite",
  ({ topic }) => (
    <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
      <Research topic={topic}>
        {(research) => (
          <Writer
            topic="latest quantum computing chips"
            research={research}
            stream={true}
          />
        )}
      </Research>
    </OpenAIProvider>
  ),
);

const workflow = gensx.Workflow("ResearchAndWriteWorkflow", ResearchAndWrite);
const stream = await workflow.run(
  { topic: "latest quantum computing chips" },
  { printUrl: true },
);

// Print the streaming response
for await (const chunk of stream) {
  process.stdout.write(chunk);
}
```

Now we can see our research step running before the write step:

![streaming](/quickstart/writer-trace.png)

While this is nice, the real power of streaming components comes when you expand or refactor your workflow. Now you could easily add an `<EditArticle>` component to the workflow that streams the response to the user with minimal changes. There's no extra plumbing needed besides removing the `stream={true}` prop on the `WriteArticle` component.

```tsx
const ResearchAndWrite = gensx.Component<{ topic: string }, string>(
  "ResearchAndWrite",
  ({ topic }) => (
    <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
      <Research topic={topic}>
        {(research) => (
        <WriteArticle topic={topic} research={research}>
          {(content) => <EditArticle content={content} stream={true}/>}
        </WriteArticle>
      )}
    </Research>
  </OpenAIProvider>,
);

const workflow = gensx.Workflow("ResearchAndWriteWorkflow", ResearchAndWrite, { printUrl: true });
const stream = await workflow.run({ topic: "latest quantum computing chips" });
```

![blog writing trace](/quickstart/blog-trace.png)

## Running the dev server

Now that you've built your workflows, you can easily turn them into REST APIs.

GenSX provides a local development server with local REST APIs that match the shape of workflows deployed to GenSX Cloud. You can run the dev server from the CLI:

```bash
# Start the development server
gensx start src/workflows.tsx
```

The development server provides several key features:

- **Hot reloading**: Changes to your code are automatically detected and recompiled
- **API endpoints**: Each workflow is exposed as a REST endpoint
- **Swagger UI**: Interactive documentation for your workflows at `http://localhost:1337/swagger-ui`
- **Local storage**: Built-in support for blob storage and databases

You'll see something like this when you start the server:

```bash
🔍 Starting GenSX Dev Server...
ℹ Starting development server...
✔ Compilation completed
✔ Generating schema

📋 Available workflows:
- ResearchAndWriteWorkflow: http://localhost:1337/workflows/ResearchAndWriteWorkflow

✅ Server is running. Press Ctrl+C to stop.
```

You can now test your workflow by sending requests to the provided URL using any HTTP client, or using the built-in Swagger UI at `http://localhost:1337/swagger-ui`.

## Deploying your project to GenSX Cloud

Now that you've tested your APIs locally, you can deploy them to the cloud. GenSX Cloud provides serverless deployment with zero configuration:

```bash
# Deploy your project to GenSX Cloud
gensx deploy src/workflows.tsx -e OPENAI_API_KEY
```

This command:

1. Builds your TypeScript code for production
2. Bundles all dependencies
3. Uploads the package to GenSX Cloud
4. Creates REST API endpoints for each workflow
5. Configures serverless infrastructure

For production deployments, you can target a specific environment:

```bash
# Deploy to production environment
gensx deploy src/index.ts --env production
```

### Running a workflow from the CLI

Once deployed, you can execute your workflows directly from the command line:

```bash
# Run a workflow synchronously
gensx run ResearchAndWriteWorkflow --input '{"topic":"quantum computing"}'

# Save the output to a file
gensx run ResearchAndWriteWorkflow --input '{"topic":"quantum computing"}' --output result.json
```

The CLI makes it easy to test your workflows and integrate them into scripts or automation.

### Running a workflow from the GenSX console

The GenSX Cloud console provides a visual interface for managing and executing your workflows:

1. Log in to [app.gensx.com](https://app.gensx.com)
2. Navigate to your project and environment
3. Select the workflow you want to run
4. Click the "Run" button and enter your input
5. View the results directly in the console

![Running a workflow in the console](/cloud/console-playground.png)

The console also provides API documentation and code snippets for your workflows as well as execution history and tracing for all previous workflow runs.

## Adding persistence with storage

Now that you've deployed your first workflow, you can add in storage so you can build more sophisticated workflows. GenSX offers three types of builtin storage: blob storage, sql database storage and, full text and vector search storage.

### Blob storage

Use the `useBlob` hook to store and retrieve unstructured data:

```tsx
import { BlobProvider, useBlob } from "@gensx/storage";

const ChatWithMemory = gensx.Component<
  { userInput: string; threadId: string },
  string
>("ChatWithMemory", async ({ userInput, threadId }) => {
  // Load chat history from cloud storage
  const blob = useBlob<ChatMessage[]>(`chat-history/${threadId}.json`);
  const history = (await blob.getJSON()) ?? [];

  // Add user message and get AI response
  // ...

  // Save updated history
  await blob.putJSON(updatedHistory);

  return response;
});

// Wrap your workflow with BlobProvider
const WorkflowComponent = gensx.Component<
  { userInput: string; threadId: string },
  string
>("Workflow", ({ userInput, threadId }) => (
  <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
    <BlobProvider>
      <ChatWithMemory userInput={userInput} threadId={threadId} />
    </BlobProvider>
  </OpenAIProvider>
));
```

### SQL database

Use the `useDatabase` hook for structured data:

```tsx
import { DatabaseProvider, useDatabase } from "@gensx/storage";

const SqlCopilot = gensx.Component<{ question: string }, string>(
  "SqlCopilot",
  async ({ question }) => {
    const db = await useDatabase("baseball");

    // Set up schema if needed
    await db.executeMultiple(`
      CREATE TABLE IF NOT EXISTS baseball_stats (
        player TEXT PRIMARY KEY,
        team TEXT,
        batting_avg REAL
      )
    `);

    // Execute query
    const result = await db.execute("SELECT * FROM baseball_stats LIMIT 5");

    // Use the result to inform your LLM response
    // ...

    return response;
  },
);

// Wrap your workflow with DatabaseProvider
const WorkflowComponent = gensx.Component<{ question: string }, string>(
  "DatabaseWorkflowComponent",
  ({ question }) => (
    <DatabaseProvider>
      <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
        <SqlCopilot question={question} />
      </OpenAIProvider>
    </DatabaseProvider>
  ),
);
```

### Full-text and vector search

Use the `useSearch` hook for semantic search and RAG applications:

```tsx
import { SearchProvider, useSearch } from "@gensx/storage";
import { OpenAIEmbedding } from "@gensx/openai";

const SemanticSearch = gensx.Component<{ query: string }, any[]>(
  "SemanticSearch",
  async ({ query }) => {
    // Access a vector search namespace
    const namespace = await useSearch("documents");

    // Generate an vector embedding for the query
    const embedding = await OpenAIEmbedding.run({
      model: "text-embedding-3-small",
      input: query,
    });

    // Search for similar documents
    const results = await namespace.query({
      vector: embedding.data[0].embedding,
      topK: 5,
      includeAttributes: true,
    });

    return results;
  },
);

// Wrap your workflow with SearchProvider
const WorkflowComponent = gensx.Component<{ query: string }, any[]>(
  "SearchWorkflow",
  ({ query }) => (
    <OpenAIProvider apiKey={process.env.OPENAI_API_KEY}>
      <SearchProvider>
        <SemanticSearch query={query} />
      </SearchProvider>
    </OpenAIProvider>
  ),
);
```

## Learning more

Explore these resources to dive deeper into GenSX:

- [Serverless Deployments](/docs/cloud/serverless-deployments): Deploy and manage your workflows in the cloud
- [Local Development](/docs/cloud/local-development): Set up a productive local environment
- [Storage Components](/docs/component-reference/storage-components): Persistent storage for your workflows
- [Observability & Tracing](/docs/cloud/observability): Debug and monitor your workflows
- [Projects & Environments](/docs/cloud/projects-environments): Organize your deployments

Check out these example projects to see GenSX in action:

- [Blog Writer](https://github.com/gensx-inc/gensx/tree/main/examples/blog-writer)
- [Chat with Memory](https://github.com/gensx-inc/gensx/tree/main/examples/chat-memory)
- [Text to SQL](https://github.com/gensx-inc/gensx/tree/main/examples/text-to-sql)
- [RAG](https://github.com/gensx-inc/gensx/tree/main/examples/rag)
